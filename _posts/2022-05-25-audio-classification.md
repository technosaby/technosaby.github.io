---
layout: splash
title: "Audio Classification"
date: 2022-05-25 7:42
header:
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: /assets/images/code.jpg
excerpt: "Audio Classification"
---


# Background

Poaching is a big problem where selfish people kill animals without knowing the consequences. Sometimes it is very difficult for forest guards to track and find the location of poaching. So Google is [working](https://https://cloud.google.com/blog/products/ai-machine-learning/how-zsl-uses-google-cloud-to-analyse-acoustic-data) with the Zoological Soceity of London to solve this problem by classifying gunshot shots. 

Humpback whales actually sing songs, which evolve over time. [Google AI is working with NOAA's Pacific Island Fisheries](https://www.blog.google/technology/ai/pattern-radio-whale-songs/) to better understand whales behavior and migratory pattern and help in protecting them. Audio Classification is a key here where it helps in identifying the correct sounds underwater.

[RedHen Labs](https://www.redhenlab.org/) is a global big data science laboratory and cooperative for research into multimodal communication. Audio is an important component in multimodal communication, so audio classification in big dataset of RedHen's News broadcats can open doors to lot of interesting research opportunities.

Use cases can be countless, so lets jump in to understand how audio classification can be done.

# Audio Classification 


# Using Yamnet


# Problems/Challenges



# References
[1] https://cloud.google.com/blog/products/ai-machine-learning/how-zsl-uses-google-cloud-to-analyse-acoustic-data
[2] https://www.blog.google/technology/ai/pattern-radio-whale-songs/

